---
title: "EC349 Individual Project Report"
author: "Hanjun Jin, 2105015"
date: "2023-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

<br>

Tabula statement

We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

Privacy statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

Related articles

Reg. 11 Academic Integrity (from 4 Oct 2021)

Guidance on Regulation 11

Proofreading Policy  

Education Policy and Quality Team

Academic Integrity (warwick.ac.uk)


<div style="page-break-after: always;"></div>

<br>

#### **Data Science Methodology**

To begin, I chose to implement John Rollin's Data Science Methodology; a 10-stage iterative process tailored for data-driven problem solving. This particular methodology aligns well with the demands of this project, given its focus on managing large data volumes, which fits our objective of accurately predicting user reviews from extensive data sets (Rollins, 2015).

<br>

```{r fig.align="center", out.width='600px', out.height='350px', echo=FALSE}
knitr::include_graphics("C:/Users/jjune/OneDrive/Desktop/EC349 Data Science for Economists/EC349 Individual Project/Appendix/John Rollin's DS Methodology Figure.jpg")
```
<center>**Figure 1**</center>

<br>

In the following sections, I will highlight all relevant stages and their utilization in developing this project. This is with the exclusion of Deployment and Feedback, as they were incorporated into the methodology with  business application in mind, which is irrelevant for this project. Data Requirements/Collection have also been ignored as the dataset was provided for me. 

<br>
<br>

#### **Problem Understanding/Analytic Approach**

This phase of the methodology involves accurately defining the task and identifying the appropriate analytic approach to be adopted. The assigned task was to use Yelp datasets to create a model predicting the User Reviews given by user *i* to business *j* in the test data. Given the ordinal nature of the 'stars', a regression model was the intuitive choice. This indicated the use of predictive analytics including regression models with shrinkage estimators and regression trees. Furthermore, the dataset provided is large and contains a variety of predictors, which indicates the need for a more flexible model as it would better capture a complex and potentially non-linear relationship. Inevitably, a more flexible model harms interpret ability, but as I am only interested in prediction, this is not a significant concern. 

<br>
<br>

#### **Data Understanding/Preparation**

Data understanding/preparation involves exploring and cleaning the data so that it becomes ready for modelling.
As I had a regression model in mind, it was integral that all variables were either dummy or continuous. The data was mostly clean, but there were variables where the data contained too many NA variables to add to the model. Most notably, the "attributes" column from business_data was a variable describing the various characteristics of the business. The variable had 4 unique observations: "True", "False", "NA" , "None". The majority of the observations were recorded as "NA", providing insufficient information for most observations. Furthermore, it was difficult to circumvent this using statistical methods as the answers themselves, "False", "NA", "None" were dubious in their meaning. The answers "None" and "False" could be argued to mean the same thing. Distributing "NA" mimicking the distribution observed from "True":"False" was also problematic as "NA" could also be interpreted as "False", skewing the true distribution. 

<br>
<br>



#### **Modelling/Evaluation**

At first glance, the most significant predictor would be thought to be the review itself. However, as the review is comprised of text it becomes difficult to implement in a regression model. To utilize this "text" variable, I ran a sentiment analysis using the "tidytext" package. Sentiment analysis involves breaking down the text to certain keywords that are matched to a pre-defined 'sentiment lexicon'. This gives a sentiment score for each keyword, which is then aggregated to give the final score of an observation from the "text" column. This has obvious limitations in that text can often be nuanced or even have a dimension of duality. Such occurrences would add noise to the model as the "sentiment score" doesn't accurately reflect the sentiment of the text, potentially worsening its performance. However, due to the importance of the text and the lack of other significant variables in the review data, it was included in my original model. To test whether sentiment analysis was beneficial I ran a copy of my original model, which was identical apart from the exclusion of sentiment_score. 

<br>

For my model, I chose to run a regression model with Lasso estimators. Shrinkage estimators were necessary as I was running a high-dimensional model with 29 predictors, I was risking potentially over fitting and capturing noise. Lasso estimators were chosen over Ridge, as I had included 29 predictors, some of which I suspected may not be significant. As such, Lasso would serve as a statistical filtering system, due to its ability to shrink certain coefficients to 0. A disadvantage of Lasso estimators is that it can be unstable when the predictors may be highly correlated. In my model, I have several predictors that may suffer from multicollinearity such as "state", "latitude" and "longitude", which by definition will be correlated to each other. In this situation Lasso tends to arbitrarily selecting one feature over another and handing over its explanatory power. This would change the meaning of this coefficient entirely, making it difficult to interpret. However, as I was only aiming for prediction, this issue could be disregarded as I would be gaining a decrease in variance with Lasso variables implying a more robust model. As I merged "user_data", "business_data" and "review_data_small", the model would take the form of 
<br>
$$
y = {Review Data} + {User Characteristics} + {Business Charactersitics} + ϵ
$$
<br> 
To chose Lambda for my Lasso estimator, I implemented 10-fold cross validation. This is done by randomly subsetting the training sample into 10 different ones, where one is chosen as the validation set and all others are used as training data. This process is repeated 10 times, with the best performing lambda in terms of MSE is selected as the optimal lambda. This allows the model to be robust and decreases the risk of a single random distribution of a training dataset influencing my entire model. For the purpose of my model, I found that the lambda chosen,i.e. significantly changed the amount of significant predictors. 

<br>

<div align="center">

|  | Lasso (lambda.min) | Lasso (lambda.1se) | Lasso w/o sentiment (lambda.min) | Lasso w/o sentiment (lambda.1se) |
|---------|---------|---------|---------|---------|
| MSE | 1.024913 | 1.035778| 1.164779 | 1.178501 |
| R^2 | 0.5248089| 0.5197715| 0.4599614| 0.4535993|


</div>

<br>

As my dataset was a subset of the larger dataset, my initial predictions were that the robust lambda chosen with the 1 s.e. criterion would outperform the min. MSE criterion, as I was concerned about overfitting. However, these metrics show that the training data was fairly similar to the test data and therefore performance was peaked with minimum MSE. Furthermore, the sentiment analysis was beneficial in predicting the "stars" of a review, despite its limitations. The best performing model, Lasso with minimum MSE criteria and sentiment analysis, was able to explain around 52% of the variance, which could be said to have a moderate performance. However, given that the scale of the data is from 1-5, a squared deviation of around 1 can be alarming. Especially since the ratings are given as integers, meaning the model could be said to be off by 25% on average. 

<br>

#### **Challenges**

The most frustrating challenge was during data preparation, when I encountered that many of my columns had "NA" observations after merging. This was a problem as these "NA" values didn't appear in the raw dataset. The explanation for this occurrence was that the 2 "small" datasets that I had merged were subsets of a larger dataset. Which meant that there were missing user ID/review ID's for both datasets that were only present in  the larger dataset. To resolve, I merged the small review dataset with the original user dataset so that all user ID's in the review dataset were accounted for. However, I was unable to solve the root of my problems; the computational limitations of my device. This was a common theme throughout this project as many methods were implemented to circumvent the limited capabilities of my device only to cause different problems. Another example was when I had tried to compute the "categories" variable as one of the predictors for my model. Mindful that the predictor would produce many parameters I removed observations using the factor 50 rule for computational and statistical reasons. However, even with removal of variables, the matrix was too complex for my device and I was unable to implement it into my model. 

<br>
<br>

#### **References**


Alwosheel, A., van Cranenburgh, S. and Chorus, C.G. (2018) ‘Is your dataset big enough? sample size requirements when using artificial neural networks for discrete choice analysis’, *Journal of Choice Modelling*, 28, pp. 167–182. doi:10.1016/j.jocm.2018.07.002. 

Kalsbeek, R. (2020) *Understanding the different types of analytics: Descriptive, diagnostic, predictive, and prescriptive, Iteration Insights.* Available at: https://iterationinsights.com/article/understanding-the-different-types-of-analytics/#:~:text=There%20are%20different%20types%20of,levels%20of%20these%20four%20categories. (Accessed: 30 November 2023). 

Logallo, N. (2019) *Data science methodology 101, Medium.* Available at: https://towardsdatascience.com/data-science-methodology-101-ce9f0d660336 (Accessed: 30 November 2023). 

Robinson, J.S. and D. (2022) *Welcome to text mining with r: Text mining with R, A Tidy Approach.* Available at: https://www.tidytextmining.com/ (Accessed: 30 November 2023). 

Rollins, J.B., 2015. *Foundational Methodology for Data Science.* IBM Analytics White Paper.

Yelp (2023) *Yelp Dataset, Yelp dataset.* Available at: https://www.yelp.com/dataset/documentation/main (Accessed: 30 November 2023). 













